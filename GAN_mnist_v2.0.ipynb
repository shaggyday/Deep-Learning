{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import time\n",
    "from tensorflow.examples.tutorials.mnist import input_data\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Activation, Flatten, Reshape\n",
    "from keras.layers import Conv2D, Conv2DTranspose, UpSampling2D, ZeroPadding2D\n",
    "from keras.layers import LeakyReLU, Dropout\n",
    "from keras.layers import BatchNormalization\n",
    "from keras.optimizers import Adam, RMSprop\n",
    "from keras.datasets import mnist\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "kernel_size = 8\n",
    "\n",
    "def generator_model():\n",
    "    dropout = 0.25\n",
    "    dim = 7\n",
    "    depth = 128\n",
    "    # In: 100\n",
    "    # Out: 7 x 7 x 128\n",
    "    model = Sequential()\n",
    "    model.add(Dense(dim*dim*depth, input_dim=100))\n",
    "    model.add(LeakyReLU())\n",
    "    model.add(Reshape((dim, dim, depth)))\n",
    "    model.add(BatchNormalization(momentum=0.8))\n",
    "    model.add(Dropout(dropout))\n",
    "    # In: 7 x 7 x 128\n",
    "    # Out: 14 x 14 x 64\n",
    "    model.add(UpSampling2D())\n",
    "    model.add(Conv2DTranspose(int(depth/2), kernel_size, padding='same'))\n",
    "    model.add(BatchNormalization(momentum=0.8))\n",
    "    model.add(LeakyReLU())\n",
    "    # In: 14 x 14 x 64\n",
    "    # Out: 28 x 28 x 32\n",
    "    model.add(UpSampling2D())\n",
    "    model.add(Conv2DTranspose(int(depth/4), kernel_size, padding='same'))\n",
    "    model.add(BatchNormalization(momentum=0.8))\n",
    "    model.add(LeakyReLU())\n",
    "    # In: 28 x 28 x 32\n",
    "    # Out: 28 x 28 x 1\n",
    "#     model.add(Conv2DTranspose(int(depth/8), 5, padding='same'))\n",
    "#     model.add(BatchNormalization(momentum=0.8))\n",
    "#     model.add(LeakyReLU())\n",
    "    model.add(Conv2DTranspose(1, kernel_size, padding='same'))\n",
    "    model.add(Activation('tanh'))\n",
    "    model.summary()\n",
    "    return model\n",
    "\n",
    "\n",
    "def discriminator_model():\n",
    "    depth = 64\n",
    "    dropout = 0.25\n",
    "    model = Sequential()\n",
    "    # In: 28 x 28 x 1, depth = 1\n",
    "    # Out: 14 x 14 x 1, depth = 64\n",
    "    model.add(Conv2D(depth, kernel_size, strides = 2, padding ='same', input_shape=(28, 28, 1)))\n",
    "    model.add(LeakyReLU(alpha=0.2))\n",
    "    model.add(Dropout(dropout))\n",
    "    \n",
    "    model.add(Conv2D(depth*2, kernel_size, strides = 2, padding ='same'))\n",
    "    model.add(ZeroPadding2D(padding=((0,1),(0,1))))\n",
    "    model.add(BatchNormalization(momentum=0.8))\n",
    "    model.add(LeakyReLU(alpha=0.2))\n",
    "    model.add(Dropout(dropout))\n",
    "    \n",
    "    model.add(Conv2D(depth*4, kernel_size, strides = 2, padding ='same'))\n",
    "    model.add(BatchNormalization(momentum=0.8))\n",
    "    model.add(LeakyReLU(alpha=0.2))\n",
    "    model.add(Dropout(dropout))\n",
    "    \n",
    "    model.add(Conv2D(depth*8, kernel_size, strides = 1, padding ='same'))\n",
    "    model.add(BatchNormalization(momentum=0.8))\n",
    "    model.add(LeakyReLU(alpha=0.2))\n",
    "    model.add(Dropout(dropout))\n",
    "    \n",
    "    # Out: 1-dim probability\n",
    "    model.add(Flatten())\n",
    "    model.add(Dense(1))\n",
    "    model.add(Activation('sigmoid'))\n",
    "    model.summary()\n",
    "    return model\n",
    "\n",
    "def adversial_model(generator, discriminator):\n",
    "    model = Sequential()\n",
    "    model.add(generator)\n",
    "    discriminator.trainable = False\n",
    "    model.add(discriminator)\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "conv2d_21 (Conv2D)           (None, 14, 14, 64)        1664      \n",
      "_________________________________________________________________\n",
      "leaky_re_lu_37 (LeakyReLU)   (None, 14, 14, 64)        0         \n",
      "_________________________________________________________________\n",
      "dropout_22 (Dropout)         (None, 14, 14, 64)        0         \n",
      "_________________________________________________________________\n",
      "conv2d_22 (Conv2D)           (None, 7, 7, 128)         204928    \n",
      "_________________________________________________________________\n",
      "zero_padding2d_6 (ZeroPaddin (None, 8, 8, 128)         0         \n",
      "_________________________________________________________________\n",
      "batch_normalization_28 (Batc (None, 8, 8, 128)         512       \n",
      "_________________________________________________________________\n",
      "leaky_re_lu_38 (LeakyReLU)   (None, 8, 8, 128)         0         \n",
      "_________________________________________________________________\n",
      "dropout_23 (Dropout)         (None, 8, 8, 128)         0         \n",
      "_________________________________________________________________\n",
      "conv2d_23 (Conv2D)           (None, 4, 4, 256)         819456    \n",
      "_________________________________________________________________\n",
      "batch_normalization_29 (Batc (None, 4, 4, 256)         1024      \n",
      "_________________________________________________________________\n",
      "leaky_re_lu_39 (LeakyReLU)   (None, 4, 4, 256)         0         \n",
      "_________________________________________________________________\n",
      "dropout_24 (Dropout)         (None, 4, 4, 256)         0         \n",
      "_________________________________________________________________\n",
      "conv2d_24 (Conv2D)           (None, 4, 4, 512)         3277312   \n",
      "_________________________________________________________________\n",
      "batch_normalization_30 (Batc (None, 4, 4, 512)         2048      \n",
      "_________________________________________________________________\n",
      "leaky_re_lu_40 (LeakyReLU)   (None, 4, 4, 512)         0         \n",
      "_________________________________________________________________\n",
      "dropout_25 (Dropout)         (None, 4, 4, 512)         0         \n",
      "_________________________________________________________________\n",
      "flatten_6 (Flatten)          (None, 8192)              0         \n",
      "_________________________________________________________________\n",
      "dense_11 (Dense)             (None, 1)                 8193      \n",
      "_________________________________________________________________\n",
      "activation_11 (Activation)   (None, 1)                 0         \n",
      "=================================================================\n",
      "Total params: 4,315,137\n",
      "Trainable params: 4,313,345\n",
      "Non-trainable params: 1,792\n",
      "_________________________________________________________________\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "dense_12 (Dense)             (None, 6272)              633472    \n",
      "_________________________________________________________________\n",
      "leaky_re_lu_41 (LeakyReLU)   (None, 6272)              0         \n",
      "_________________________________________________________________\n",
      "reshape_6 (Reshape)          (None, 7, 7, 128)         0         \n",
      "_________________________________________________________________\n",
      "batch_normalization_31 (Batc (None, 7, 7, 128)         512       \n",
      "_________________________________________________________________\n",
      "dropout_26 (Dropout)         (None, 7, 7, 128)         0         \n",
      "_________________________________________________________________\n",
      "up_sampling2d_11 (UpSampling (None, 14, 14, 128)       0         \n",
      "_________________________________________________________________\n",
      "conv2d_transpose_17 (Conv2DT (None, 14, 14, 64)        204864    \n",
      "_________________________________________________________________\n",
      "batch_normalization_32 (Batc (None, 14, 14, 64)        256       \n",
      "_________________________________________________________________\n",
      "leaky_re_lu_42 (LeakyReLU)   (None, 14, 14, 64)        0         \n",
      "_________________________________________________________________\n",
      "up_sampling2d_12 (UpSampling (None, 28, 28, 64)        0         \n",
      "_________________________________________________________________\n",
      "conv2d_transpose_18 (Conv2DT (None, 28, 28, 32)        51232     \n",
      "_________________________________________________________________\n",
      "batch_normalization_33 (Batc (None, 28, 28, 32)        128       \n",
      "_________________________________________________________________\n",
      "leaky_re_lu_43 (LeakyReLU)   (None, 28, 28, 32)        0         \n",
      "_________________________________________________________________\n",
      "conv2d_transpose_19 (Conv2DT (None, 28, 28, 1)         801       \n",
      "_________________________________________________________________\n",
      "activation_12 (Activation)   (None, 28, 28, 1)         0         \n",
      "=================================================================\n",
      "Total params: 891,265\n",
      "Trainable params: 890,817\n",
      "Non-trainable params: 448\n",
      "_________________________________________________________________\n",
      "0: [D loss: 0.851484, acc: 0.500000]  [A loss: 1.437345, acc: 0.218750]\n",
      "1: [D loss: 1.293184, acc: 0.250000]  [A loss: 1.192220, acc: 0.250000]\n",
      "2: [D loss: 1.221658, acc: 0.265625]  [A loss: 1.178433, acc: 0.343750]\n",
      "3: [D loss: 1.343053, acc: 0.187500]  [A loss: 1.003671, acc: 0.375000]\n",
      "4: [D loss: 1.206197, acc: 0.296875]  [A loss: 0.954951, acc: 0.437500]\n",
      "5: [D loss: 1.229192, acc: 0.218750]  [A loss: 1.144259, acc: 0.375000]\n",
      "6: [D loss: 1.088059, acc: 0.312500]  [A loss: 0.987087, acc: 0.375000]\n",
      "7: [D loss: 1.251031, acc: 0.203125]  [A loss: 1.005530, acc: 0.312500]\n",
      "8: [D loss: 1.117714, acc: 0.265625]  [A loss: 1.050576, acc: 0.281250]\n",
      "9: [D loss: 1.169548, acc: 0.343750]  [A loss: 0.910624, acc: 0.375000]\n",
      "10: [D loss: 1.224424, acc: 0.234375]  [A loss: 1.003717, acc: 0.343750]\n",
      "11: [D loss: 1.096254, acc: 0.375000]  [A loss: 0.858588, acc: 0.437500]\n",
      "12: [D loss: 1.220377, acc: 0.234375]  [A loss: 1.124612, acc: 0.218750]\n",
      "13: [D loss: 1.222340, acc: 0.250000]  [A loss: 0.871900, acc: 0.343750]\n",
      "14: [D loss: 1.047752, acc: 0.359375]  [A loss: 1.049009, acc: 0.343750]\n",
      "15: [D loss: 0.915319, acc: 0.453125]  [A loss: 0.749305, acc: 0.562500]\n",
      "16: [D loss: 1.116517, acc: 0.265625]  [A loss: 0.947439, acc: 0.437500]\n",
      "17: [D loss: 1.137280, acc: 0.296875]  [A loss: 1.019211, acc: 0.375000]\n",
      "18: [D loss: 1.088776, acc: 0.265625]  [A loss: 0.839979, acc: 0.500000]\n",
      "19: [D loss: 1.089995, acc: 0.390625]  [A loss: 0.910450, acc: 0.437500]\n",
      "20: [D loss: 0.944704, acc: 0.453125]  [A loss: 0.891674, acc: 0.500000]\n",
      "21: [D loss: 1.113072, acc: 0.343750]  [A loss: 0.967943, acc: 0.437500]\n",
      "22: [D loss: 1.156011, acc: 0.343750]  [A loss: 0.788840, acc: 0.593750]\n",
      "23: [D loss: 1.076570, acc: 0.390625]  [A loss: 0.737401, acc: 0.531250]\n",
      "24: [D loss: 0.904316, acc: 0.437500]  [A loss: 0.825029, acc: 0.562500]\n",
      "25: [D loss: 1.009177, acc: 0.390625]  [A loss: 0.786966, acc: 0.500000]\n",
      "26: [D loss: 1.124855, acc: 0.421875]  [A loss: 0.734611, acc: 0.625000]\n",
      "27: [D loss: 0.897315, acc: 0.390625]  [A loss: 0.702932, acc: 0.593750]\n",
      "28: [D loss: 0.995460, acc: 0.359375]  [A loss: 0.789957, acc: 0.468750]\n",
      "29: [D loss: 0.925282, acc: 0.406250]  [A loss: 0.707758, acc: 0.593750]\n",
      "30: [D loss: 1.046928, acc: 0.421875]  [A loss: 0.775429, acc: 0.593750]\n",
      "31: [D loss: 0.993877, acc: 0.343750]  [A loss: 0.668620, acc: 0.625000]\n",
      "32: [D loss: 0.978895, acc: 0.375000]  [A loss: 0.864622, acc: 0.437500]\n",
      "33: [D loss: 1.087858, acc: 0.359375]  [A loss: 0.791929, acc: 0.562500]\n",
      "34: [D loss: 0.977601, acc: 0.437500]  [A loss: 0.567040, acc: 0.687500]\n",
      "35: [D loss: 0.962402, acc: 0.500000]  [A loss: 0.765990, acc: 0.562500]\n",
      "36: [D loss: 0.994138, acc: 0.468750]  [A loss: 0.529827, acc: 0.718750]\n",
      "37: [D loss: 0.957844, acc: 0.406250]  [A loss: 0.751254, acc: 0.500000]\n",
      "38: [D loss: 1.089222, acc: 0.390625]  [A loss: 0.748060, acc: 0.468750]\n",
      "39: [D loss: 0.898727, acc: 0.468750]  [A loss: 0.579457, acc: 0.750000]\n",
      "40: [D loss: 0.842107, acc: 0.500000]  [A loss: 0.547935, acc: 0.718750]\n",
      "41: [D loss: 1.171419, acc: 0.453125]  [A loss: 0.760047, acc: 0.562500]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "42: [D loss: 1.069014, acc: 0.453125]  [A loss: 0.599927, acc: 0.687500]\n",
      "43: [D loss: 0.935220, acc: 0.437500]  [A loss: 0.521543, acc: 0.718750]\n",
      "44: [D loss: 0.958224, acc: 0.484375]  [A loss: 0.752941, acc: 0.593750]\n",
      "45: [D loss: 0.893915, acc: 0.484375]  [A loss: 0.731355, acc: 0.687500]\n",
      "46: [D loss: 0.836973, acc: 0.546875]  [A loss: 0.514563, acc: 0.812500]\n",
      "47: [D loss: 1.037406, acc: 0.437500]  [A loss: 0.621185, acc: 0.656250]\n",
      "48: [D loss: 0.932500, acc: 0.453125]  [A loss: 0.495234, acc: 0.812500]\n",
      "49: [D loss: 0.959106, acc: 0.421875]  [A loss: 0.662913, acc: 0.625000]\n",
      "50: [D loss: 0.931294, acc: 0.437500]  [A loss: 0.520612, acc: 0.750000]\n",
      "51: [D loss: 1.030252, acc: 0.406250]  [A loss: 0.419340, acc: 0.812500]\n",
      "52: [D loss: 0.946413, acc: 0.515625]  [A loss: 0.585875, acc: 0.656250]\n",
      "53: [D loss: 0.911421, acc: 0.453125]  [A loss: 0.546496, acc: 0.718750]\n",
      "54: [D loss: 0.957036, acc: 0.468750]  [A loss: 0.500673, acc: 0.812500]\n",
      "55: [D loss: 0.827017, acc: 0.562500]  [A loss: 0.571565, acc: 0.781250]\n",
      "56: [D loss: 0.921378, acc: 0.421875]  [A loss: 0.480688, acc: 0.718750]\n",
      "57: [D loss: 0.991448, acc: 0.453125]  [A loss: 0.568091, acc: 0.687500]\n",
      "58: [D loss: 1.006052, acc: 0.421875]  [A loss: 0.436186, acc: 0.812500]\n",
      "59: [D loss: 0.830179, acc: 0.546875]  [A loss: 0.480212, acc: 0.750000]\n",
      "60: [D loss: 0.890623, acc: 0.531250]  [A loss: 0.492251, acc: 0.843750]\n",
      "61: [D loss: 0.940489, acc: 0.500000]  [A loss: 0.461483, acc: 0.781250]\n",
      "62: [D loss: 0.862477, acc: 0.515625]  [A loss: 0.544988, acc: 0.781250]\n",
      "63: [D loss: 1.014527, acc: 0.421875]  [A loss: 0.413126, acc: 0.875000]\n",
      "64: [D loss: 0.860622, acc: 0.484375]  [A loss: 0.489511, acc: 0.843750]\n",
      "65: [D loss: 1.042452, acc: 0.468750]  [A loss: 0.545426, acc: 0.687500]\n",
      "66: [D loss: 0.869434, acc: 0.562500]  [A loss: 0.454012, acc: 0.843750]\n",
      "67: [D loss: 0.910080, acc: 0.500000]  [A loss: 0.309040, acc: 0.968750]\n",
      "68: [D loss: 0.862656, acc: 0.562500]  [A loss: 0.416173, acc: 0.812500]\n",
      "69: [D loss: 0.880620, acc: 0.453125]  [A loss: 0.467207, acc: 0.875000]\n",
      "70: [D loss: 0.845000, acc: 0.531250]  [A loss: 0.447416, acc: 0.843750]\n",
      "71: [D loss: 0.918473, acc: 0.453125]  [A loss: 0.380598, acc: 0.875000]\n",
      "72: [D loss: 0.977139, acc: 0.453125]  [A loss: 0.539981, acc: 0.687500]\n",
      "73: [D loss: 0.909057, acc: 0.500000]  [A loss: 0.525092, acc: 0.687500]\n",
      "74: [D loss: 0.956471, acc: 0.515625]  [A loss: 0.468768, acc: 0.750000]\n",
      "75: [D loss: 0.835477, acc: 0.515625]  [A loss: 0.328001, acc: 0.875000]\n",
      "76: [D loss: 0.900059, acc: 0.500000]  [A loss: 0.450881, acc: 0.781250]\n",
      "77: [D loss: 0.937681, acc: 0.484375]  [A loss: 0.416199, acc: 0.812500]\n",
      "78: [D loss: 0.863528, acc: 0.515625]  [A loss: 0.329870, acc: 0.906250]\n",
      "79: [D loss: 0.761827, acc: 0.546875]  [A loss: 0.432969, acc: 0.875000]\n",
      "80: [D loss: 0.916240, acc: 0.484375]  [A loss: 0.411108, acc: 0.843750]\n",
      "81: [D loss: 0.936921, acc: 0.593750]  [A loss: 0.431525, acc: 0.843750]\n",
      "82: [D loss: 0.789832, acc: 0.640625]  [A loss: 0.466122, acc: 0.843750]\n",
      "83: [D loss: 0.845280, acc: 0.531250]  [A loss: 0.409498, acc: 0.875000]\n",
      "84: [D loss: 0.884468, acc: 0.500000]  [A loss: 0.330684, acc: 0.968750]\n",
      "85: [D loss: 0.870270, acc: 0.531250]  [A loss: 0.349521, acc: 0.843750]\n",
      "86: [D loss: 0.864187, acc: 0.531250]  [A loss: 0.377143, acc: 0.843750]\n",
      "87: [D loss: 0.813546, acc: 0.562500]  [A loss: 0.342992, acc: 0.875000]\n",
      "88: [D loss: 0.797290, acc: 0.531250]  [A loss: 0.453427, acc: 0.875000]\n",
      "89: [D loss: 0.806675, acc: 0.531250]  [A loss: 0.446449, acc: 0.750000]\n",
      "90: [D loss: 0.802811, acc: 0.546875]  [A loss: 0.394462, acc: 0.812500]\n",
      "91: [D loss: 0.810897, acc: 0.625000]  [A loss: 0.425360, acc: 0.812500]\n",
      "92: [D loss: 0.897281, acc: 0.484375]  [A loss: 0.478638, acc: 0.843750]\n",
      "93: [D loss: 0.727161, acc: 0.562500]  [A loss: 0.319566, acc: 0.937500]\n",
      "94: [D loss: 0.927238, acc: 0.500000]  [A loss: 0.393175, acc: 0.843750]\n",
      "95: [D loss: 0.930958, acc: 0.546875]  [A loss: 0.418185, acc: 0.812500]\n",
      "96: [D loss: 0.930194, acc: 0.453125]  [A loss: 0.332154, acc: 0.875000]\n",
      "97: [D loss: 0.789426, acc: 0.625000]  [A loss: 0.339085, acc: 0.937500]\n",
      "98: [D loss: 0.869549, acc: 0.546875]  [A loss: 0.334646, acc: 0.968750]\n",
      "99: [D loss: 0.763583, acc: 0.578125]  [A loss: 0.369229, acc: 0.875000]\n",
      "100: [D loss: 0.757640, acc: 0.593750]  [A loss: 0.276669, acc: 0.906250]\n",
      "101: [D loss: 0.892548, acc: 0.500000]  [A loss: 0.404218, acc: 0.781250]\n",
      "102: [D loss: 0.776886, acc: 0.562500]  [A loss: 0.306941, acc: 0.937500]\n",
      "103: [D loss: 0.771295, acc: 0.531250]  [A loss: 0.354507, acc: 0.906250]\n",
      "104: [D loss: 0.887772, acc: 0.468750]  [A loss: 0.314234, acc: 0.875000]\n",
      "105: [D loss: 0.764989, acc: 0.625000]  [A loss: 0.313508, acc: 0.875000]\n",
      "106: [D loss: 0.865462, acc: 0.546875]  [A loss: 0.296555, acc: 0.906250]\n",
      "107: [D loss: 0.733178, acc: 0.640625]  [A loss: 0.336686, acc: 0.875000]\n",
      "108: [D loss: 0.770685, acc: 0.593750]  [A loss: 0.413232, acc: 0.875000]\n",
      "109: [D loss: 0.818738, acc: 0.609375]  [A loss: 0.326466, acc: 0.906250]\n",
      "110: [D loss: 0.818063, acc: 0.578125]  [A loss: 0.369155, acc: 0.875000]\n",
      "111: [D loss: 0.826870, acc: 0.500000]  [A loss: 0.299492, acc: 0.937500]\n",
      "112: [D loss: 0.704519, acc: 0.625000]  [A loss: 0.273615, acc: 0.906250]\n",
      "113: [D loss: 0.787661, acc: 0.468750]  [A loss: 0.238723, acc: 0.937500]\n",
      "114: [D loss: 0.864802, acc: 0.562500]  [A loss: 0.348058, acc: 0.875000]\n",
      "115: [D loss: 0.789771, acc: 0.562500]  [A loss: 0.352572, acc: 0.875000]\n",
      "116: [D loss: 0.832853, acc: 0.531250]  [A loss: 0.260830, acc: 0.906250]\n",
      "117: [D loss: 0.759900, acc: 0.609375]  [A loss: 0.245549, acc: 1.000000]\n",
      "118: [D loss: 0.842910, acc: 0.546875]  [A loss: 0.279378, acc: 0.875000]\n",
      "119: [D loss: 0.833893, acc: 0.500000]  [A loss: 0.300837, acc: 0.937500]\n",
      "120: [D loss: 0.872051, acc: 0.578125]  [A loss: 0.229427, acc: 1.000000]\n",
      "121: [D loss: 0.734765, acc: 0.578125]  [A loss: 0.319916, acc: 0.906250]\n",
      "122: [D loss: 0.724889, acc: 0.578125]  [A loss: 0.283486, acc: 0.937500]\n",
      "123: [D loss: 0.740692, acc: 0.546875]  [A loss: 0.293916, acc: 0.968750]\n",
      "124: [D loss: 0.724945, acc: 0.609375]  [A loss: 0.224540, acc: 0.968750]\n",
      "125: [D loss: 0.817043, acc: 0.593750]  [A loss: 0.258034, acc: 0.968750]\n",
      "126: [D loss: 0.794184, acc: 0.546875]  [A loss: 0.211248, acc: 1.000000]\n",
      "127: [D loss: 0.752326, acc: 0.546875]  [A loss: 0.248800, acc: 0.937500]\n",
      "128: [D loss: 0.791420, acc: 0.593750]  [A loss: 0.258131, acc: 0.937500]\n",
      "129: [D loss: 0.833678, acc: 0.531250]  [A loss: 0.346241, acc: 0.906250]\n",
      "130: [D loss: 0.781928, acc: 0.531250]  [A loss: 0.309099, acc: 0.937500]\n",
      "131: [D loss: 0.819858, acc: 0.593750]  [A loss: 0.288040, acc: 0.906250]\n",
      "132: [D loss: 0.779331, acc: 0.593750]  [A loss: 0.225024, acc: 1.000000]\n",
      "133: [D loss: 0.813454, acc: 0.531250]  [A loss: 0.264928, acc: 0.906250]\n",
      "134: [D loss: 0.831731, acc: 0.562500]  [A loss: 0.215411, acc: 0.968750]\n",
      "135: [D loss: 0.774950, acc: 0.593750]  [A loss: 0.297138, acc: 0.906250]\n",
      "136: [D loss: 0.887164, acc: 0.562500]  [A loss: 0.264261, acc: 0.906250]\n",
      "137: [D loss: 0.772441, acc: 0.562500]  [A loss: 0.250410, acc: 1.000000]\n",
      "138: [D loss: 0.816886, acc: 0.562500]  [A loss: 0.145259, acc: 1.000000]\n",
      "139: [D loss: 0.782980, acc: 0.546875]  [A loss: 0.198266, acc: 1.000000]\n",
      "140: [D loss: 0.760067, acc: 0.640625]  [A loss: 0.244039, acc: 0.937500]\n",
      "141: [D loss: 0.690178, acc: 0.656250]  [A loss: 0.241333, acc: 0.937500]\n",
      "142: [D loss: 0.723374, acc: 0.562500]  [A loss: 0.323497, acc: 0.906250]\n",
      "143: [D loss: 0.748057, acc: 0.578125]  [A loss: 0.235568, acc: 0.968750]\n",
      "144: [D loss: 0.713617, acc: 0.578125]  [A loss: 0.176430, acc: 1.000000]\n",
      "145: [D loss: 0.735186, acc: 0.546875]  [A loss: 0.215729, acc: 0.968750]\n",
      "146: [D loss: 0.799446, acc: 0.578125]  [A loss: 0.236493, acc: 0.937500]\n",
      "147: [D loss: 0.718100, acc: 0.625000]  [A loss: 0.272034, acc: 0.968750]\n",
      "148: [D loss: 0.790387, acc: 0.578125]  [A loss: 0.235513, acc: 0.937500]\n",
      "149: [D loss: 0.750617, acc: 0.562500]  [A loss: 0.226671, acc: 1.000000]\n",
      "150: [D loss: 0.842101, acc: 0.546875]  [A loss: 0.319288, acc: 0.906250]\n",
      "151: [D loss: 0.714364, acc: 0.609375]  [A loss: 0.177312, acc: 0.968750]\n",
      "152: [D loss: 0.856484, acc: 0.531250]  [A loss: 0.226453, acc: 0.937500]\n",
      "153: [D loss: 0.712675, acc: 0.562500]  [A loss: 0.215598, acc: 0.937500]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "154: [D loss: 0.775846, acc: 0.578125]  [A loss: 0.226392, acc: 1.000000]\n",
      "155: [D loss: 0.769605, acc: 0.593750]  [A loss: 0.172759, acc: 1.000000]\n",
      "156: [D loss: 0.841770, acc: 0.562500]  [A loss: 0.166823, acc: 0.968750]\n",
      "157: [D loss: 0.689434, acc: 0.609375]  [A loss: 0.210119, acc: 0.968750]\n",
      "158: [D loss: 0.680800, acc: 0.578125]  [A loss: 0.205139, acc: 0.968750]\n",
      "159: [D loss: 0.790789, acc: 0.578125]  [A loss: 0.305084, acc: 0.968750]\n",
      "160: [D loss: 0.701319, acc: 0.640625]  [A loss: 0.202645, acc: 0.968750]\n",
      "161: [D loss: 0.782106, acc: 0.515625]  [A loss: 0.228501, acc: 0.906250]\n",
      "162: [D loss: 0.736633, acc: 0.609375]  [A loss: 0.246057, acc: 0.937500]\n",
      "163: [D loss: 0.754763, acc: 0.546875]  [A loss: 0.162347, acc: 1.000000]\n",
      "164: [D loss: 0.776791, acc: 0.546875]  [A loss: 0.224622, acc: 1.000000]\n",
      "165: [D loss: 0.724798, acc: 0.578125]  [A loss: 0.188106, acc: 1.000000]\n",
      "166: [D loss: 0.836904, acc: 0.546875]  [A loss: 0.280093, acc: 0.906250]\n",
      "167: [D loss: 0.637545, acc: 0.593750]  [A loss: 0.219327, acc: 0.937500]\n",
      "168: [D loss: 0.658918, acc: 0.625000]  [A loss: 0.171790, acc: 1.000000]\n",
      "169: [D loss: 0.756137, acc: 0.593750]  [A loss: 0.227601, acc: 0.968750]\n",
      "170: [D loss: 0.738698, acc: 0.562500]  [A loss: 0.203640, acc: 0.937500]\n",
      "171: [D loss: 0.600882, acc: 0.656250]  [A loss: 0.213851, acc: 0.937500]\n",
      "172: [D loss: 0.738302, acc: 0.562500]  [A loss: 0.183448, acc: 0.968750]\n",
      "173: [D loss: 0.674308, acc: 0.593750]  [A loss: 0.168375, acc: 1.000000]\n",
      "174: [D loss: 0.771432, acc: 0.562500]  [A loss: 0.217774, acc: 0.937500]\n",
      "175: [D loss: 0.802407, acc: 0.593750]  [A loss: 0.168161, acc: 1.000000]\n",
      "176: [D loss: 0.787943, acc: 0.593750]  [A loss: 0.229038, acc: 0.937500]\n",
      "177: [D loss: 0.748453, acc: 0.546875]  [A loss: 0.166730, acc: 0.937500]\n",
      "178: [D loss: 0.743768, acc: 0.578125]  [A loss: 0.171996, acc: 1.000000]\n",
      "179: [D loss: 0.773387, acc: 0.640625]  [A loss: 0.182503, acc: 0.968750]\n",
      "180: [D loss: 0.817278, acc: 0.562500]  [A loss: 0.196376, acc: 0.968750]\n",
      "181: [D loss: 0.704180, acc: 0.593750]  [A loss: 0.138306, acc: 0.968750]\n",
      "182: [D loss: 0.597455, acc: 0.656250]  [A loss: 0.176684, acc: 0.968750]\n",
      "183: [D loss: 0.666466, acc: 0.656250]  [A loss: 0.165779, acc: 1.000000]\n",
      "184: [D loss: 0.691056, acc: 0.609375]  [A loss: 0.171731, acc: 0.968750]\n",
      "185: [D loss: 0.731870, acc: 0.562500]  [A loss: 0.242509, acc: 0.937500]\n",
      "186: [D loss: 0.790356, acc: 0.562500]  [A loss: 0.221377, acc: 0.937500]\n",
      "187: [D loss: 0.724451, acc: 0.578125]  [A loss: 0.145116, acc: 1.000000]\n",
      "188: [D loss: 0.755350, acc: 0.609375]  [A loss: 0.230575, acc: 0.906250]\n",
      "189: [D loss: 0.680856, acc: 0.625000]  [A loss: 0.128992, acc: 1.000000]\n",
      "190: [D loss: 0.712705, acc: 0.609375]  [A loss: 0.177796, acc: 0.968750]\n",
      "191: [D loss: 0.790292, acc: 0.531250]  [A loss: 0.138835, acc: 0.968750]\n",
      "192: [D loss: 0.758706, acc: 0.546875]  [A loss: 0.158568, acc: 1.000000]\n",
      "193: [D loss: 0.737564, acc: 0.609375]  [A loss: 0.227127, acc: 0.875000]\n",
      "194: [D loss: 0.663750, acc: 0.625000]  [A loss: 0.161438, acc: 0.968750]\n",
      "195: [D loss: 0.823489, acc: 0.531250]  [A loss: 0.132284, acc: 1.000000]\n",
      "196: [D loss: 0.798923, acc: 0.578125]  [A loss: 0.194773, acc: 1.000000]\n",
      "197: [D loss: 0.667919, acc: 0.593750]  [A loss: 0.193105, acc: 0.968750]\n",
      "198: [D loss: 0.661067, acc: 0.609375]  [A loss: 0.177913, acc: 1.000000]\n",
      "199: [D loss: 0.756115, acc: 0.546875]  [A loss: 0.141665, acc: 1.000000]\n",
      "200: [D loss: 0.745720, acc: 0.531250]  [A loss: 0.177212, acc: 1.000000]\n",
      "201: [D loss: 0.831192, acc: 0.546875]  [A loss: 0.160787, acc: 1.000000]\n",
      "202: [D loss: 0.808688, acc: 0.546875]  [A loss: 0.180822, acc: 0.937500]\n",
      "203: [D loss: 0.729058, acc: 0.593750]  [A loss: 0.175581, acc: 0.968750]\n",
      "204: [D loss: 0.723979, acc: 0.562500]  [A loss: 0.172568, acc: 1.000000]\n",
      "205: [D loss: 0.662579, acc: 0.656250]  [A loss: 0.126630, acc: 1.000000]\n",
      "206: [D loss: 0.806169, acc: 0.578125]  [A loss: 0.155337, acc: 1.000000]\n",
      "207: [D loss: 0.775402, acc: 0.546875]  [A loss: 0.119962, acc: 1.000000]\n",
      "208: [D loss: 0.713336, acc: 0.593750]  [A loss: 0.151505, acc: 0.968750]\n",
      "209: [D loss: 0.911783, acc: 0.500000]  [A loss: 0.123781, acc: 1.000000]\n",
      "210: [D loss: 0.731014, acc: 0.593750]  [A loss: 0.185298, acc: 0.968750]\n",
      "211: [D loss: 0.751931, acc: 0.562500]  [A loss: 0.127486, acc: 1.000000]\n",
      "212: [D loss: 0.689799, acc: 0.578125]  [A loss: 0.133872, acc: 1.000000]\n",
      "213: [D loss: 0.645204, acc: 0.593750]  [A loss: 0.147975, acc: 1.000000]\n",
      "214: [D loss: 0.848456, acc: 0.562500]  [A loss: 0.133661, acc: 1.000000]\n",
      "215: [D loss: 0.738122, acc: 0.640625]  [A loss: 0.153009, acc: 0.968750]\n",
      "216: [D loss: 0.794932, acc: 0.593750]  [A loss: 0.152640, acc: 1.000000]\n",
      "217: [D loss: 0.761698, acc: 0.546875]  [A loss: 0.133711, acc: 1.000000]\n",
      "218: [D loss: 0.901332, acc: 0.578125]  [A loss: 0.141637, acc: 1.000000]\n",
      "219: [D loss: 0.730085, acc: 0.593750]  [A loss: 0.165198, acc: 1.000000]\n",
      "220: [D loss: 0.772622, acc: 0.609375]  [A loss: 0.118822, acc: 1.000000]\n",
      "221: [D loss: 0.770854, acc: 0.578125]  [A loss: 0.174323, acc: 0.968750]\n",
      "222: [D loss: 0.732139, acc: 0.546875]  [A loss: 0.107097, acc: 1.000000]\n",
      "223: [D loss: 0.765716, acc: 0.578125]  [A loss: 0.111919, acc: 1.000000]\n",
      "224: [D loss: 0.765455, acc: 0.578125]  [A loss: 0.103043, acc: 1.000000]\n",
      "225: [D loss: 0.779424, acc: 0.546875]  [A loss: 0.135110, acc: 1.000000]\n",
      "226: [D loss: 0.768920, acc: 0.562500]  [A loss: 0.093575, acc: 1.000000]\n",
      "227: [D loss: 0.728948, acc: 0.578125]  [A loss: 0.180003, acc: 0.968750]\n",
      "228: [D loss: 0.768256, acc: 0.562500]  [A loss: 0.127358, acc: 1.000000]\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-11-497d7d9d30c7>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     95\u001b[0m                 \u001b[0mplt\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msavefig\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfilename\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     96\u001b[0m                 \u001b[0mplt\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mclose\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'all'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 97\u001b[1;33m \u001b[0mtrain\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mepochs\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;36m4000\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mbatch_size\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;36m32\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0msave_interval\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;36m50\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32m<ipython-input-11-497d7d9d30c7>\u001b[0m in \u001b[0;36mtrain\u001b[1;34m(epochs, batch_size, save_interval)\u001b[0m\n\u001b[0;32m     69\u001b[0m         \u001b[0mreal\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mzeros\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mbatch_size\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m1\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     70\u001b[0m \u001b[1;31m#         discriminator.trainable = False # freeze the discriminator\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 71\u001b[1;33m         \u001b[0ma_loss\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0madversial\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtrain_on_batch\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mrandom_latent_vectors\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mreal\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     72\u001b[0m \u001b[1;31m#         discriminator.trainable = True # unfreeze the discrminator\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     73\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\keras\\engine\\training.py\u001b[0m in \u001b[0;36mtrain_on_batch\u001b[1;34m(self, x, y, sample_weight, class_weight)\u001b[0m\n\u001b[0;32m   1215\u001b[0m             \u001b[0mins\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mx\u001b[0m \u001b[1;33m+\u001b[0m \u001b[0my\u001b[0m \u001b[1;33m+\u001b[0m \u001b[0msample_weights\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1216\u001b[0m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_make_train_function\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1217\u001b[1;33m         \u001b[0moutputs\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtrain_function\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mins\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1218\u001b[0m         \u001b[1;32mreturn\u001b[0m \u001b[0munpack_singleton\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0moutputs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1219\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\keras\\backend\\tensorflow_backend.py\u001b[0m in \u001b[0;36m__call__\u001b[1;34m(self, inputs)\u001b[0m\n\u001b[0;32m   2713\u001b[0m                 \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_legacy_call\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2714\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 2715\u001b[1;33m             \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_call\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   2716\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2717\u001b[0m             \u001b[1;32mif\u001b[0m \u001b[0mpy_any\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mis_tensor\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mx\u001b[0m \u001b[1;32min\u001b[0m \u001b[0minputs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\keras\\backend\\tensorflow_backend.py\u001b[0m in \u001b[0;36m_call\u001b[1;34m(self, inputs)\u001b[0m\n\u001b[0;32m   2673\u001b[0m             \u001b[0mfetched\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_callable_fn\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0marray_vals\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mrun_metadata\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mrun_metadata\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2674\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 2675\u001b[1;33m             \u001b[0mfetched\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_callable_fn\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0marray_vals\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   2676\u001b[0m         \u001b[1;32mreturn\u001b[0m \u001b[0mfetched\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;33m:\u001b[0m\u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0moutputs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2677\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\client\\session.py\u001b[0m in \u001b[0;36m__call__\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1437\u001b[0m           ret = tf_session.TF_SessionRunCallable(\n\u001b[0;32m   1438\u001b[0m               \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_session\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_session\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_handle\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mstatus\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1439\u001b[1;33m               run_metadata_ptr)\n\u001b[0m\u001b[0;32m   1440\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mrun_metadata\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1441\u001b[0m           \u001b[0mproto_data\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtf_session\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mTF_GetBuffer\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mrun_metadata_ptr\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "def train(epochs = 1000, batch_size = 128, save_interval = 50):\n",
    "    latent_dim = 100\n",
    "    img_cols, img_rows = 28, 28\n",
    "    # Initialize data and models\n",
    "    (X_train, y_train), (X_test, y_test) = mnist.load_data() \n",
    "    X_train = X_train / 127.5 - 1.\n",
    "    X_train = np.expand_dims(X_train, axis=3)\n",
    "    discriminator = discriminator_model()\n",
    "    generator = generator_model()\n",
    "    adversial = adversial_model(generator, discriminator) # discriminator is frozen\n",
    "    g_optim = Adam(lr=0.0002, beta_1=0.5)\n",
    "    d_optim = Adam(lr=0.0002, beta_1=0.5)\n",
    "#     generator.compile(loss='binary_crossentropy', optimizer=\"RMSprop\")\n",
    "    adversial.compile(loss='binary_crossentropy', optimizer=g_optim, metrics=['accuracy'])\n",
    "#     discriminator.trainable = True # unfreeze discriminator\n",
    "    discriminator.compile(loss='binary_crossentropy', optimizer=d_optim, metrics=['accuracy'])\n",
    "    \n",
    "    # labels\n",
    "    real = np.ones((batch_size, 1))\n",
    "    fake = np.zeros((batch_size, 1))\n",
    "    \n",
    "    # Stuff for saving images\n",
    "    filename = 'mnist.png'\n",
    "    noise_input = None\n",
    "    if save_interval > 0:\n",
    "        noise_input = np.random.uniform(-1, 1, size=[16, latent_dim])\n",
    "    \n",
    "    # The training loop\n",
    "    for epoch in range(epochs):\n",
    "        # Select a random half of images\n",
    "        index = np.random.randint(0, X_train.shape[0], batch_size)\n",
    "        real_images = X_train[index]\n",
    "\n",
    "        # Sample noise and generate a batch of new images\n",
    "        noise = np.random.normal(0, 1, (batch_size, latent_dim))\n",
    "        fake_images = generator.predict(noise)\n",
    "        \n",
    "        # Train on soft labels (add noise to labels as well)\n",
    "        noise_prop = 0.05 # Randomly flip 5% of labels\n",
    "\n",
    "        # Prepare labels for real data\n",
    "#         real = np.zeros((batch_size, 1)) + np.random.uniform(low=0.0, high=0.1, size=(batch_size, 1))\n",
    "#         flipped_idx = np.random.choice(np.arange(len(real)), size=int(noise_prop*len(real)))\n",
    "#         real[flipped_idx] = 1 - real[flipped_idx]\n",
    "\n",
    "        d_loss_real = discriminator.train_on_batch(real_images, real)\n",
    "        \n",
    "        # Prepare labels for fake data\n",
    "#         fake = np.ones((batch_size, 1)) - np.random.uniform(low=0.0, high=0.1, size=(batch_size, 1))\n",
    "#         flipped_idx = np.random.choice(np.arange(len(fake)), size=int(noise_prop*len(fake)))\n",
    "#         fake[flipped_idx] = 1 - fake[flipped_idx]\n",
    "        \n",
    "        # Train the discriminator (real classified as ones and generated as zeros)\n",
    "        d_loss_fake = discriminator.train_on_batch(fake_images, fake)\n",
    "        d_loss = 0.5 * np.add(d_loss_real, d_loss_fake)\n",
    "        \n",
    "#         real_images = X_train[np.random.randint(0, X_train.shape[0], size=batch_size), :, :, :]\n",
    "#         noise = np.random.uniform(-1, 1, size=[batch_size, latent_dim])\n",
    "#         fake_images = generator.predict(noise)\n",
    "#         combined_images = np.concatenate((real_images, fake_images))\n",
    "#         labels = np.ones([2*batch_size, 1])\n",
    "#         labels[batch_size:, :] = 0\n",
    "#         labels = np.concatenate([np.ones((batch_size, 1)), np.zeros((batch_size, 1))])\n",
    "#         labels += 0.05 * np.random.random(labels.shape)\n",
    "#         d_loss = discriminator.train_on_batch(combined_images, labels)\n",
    "                                        \n",
    "#         misleading_targets = np.ones([batch_size, 1])\n",
    "        random_latent_vectors = np.random.normal(0, 1, (batch_size, latent_dim))\n",
    "        real = np.zeros((batch_size, 1))\n",
    "#         discriminator.trainable = False # freeze the discriminator\n",
    "        a_loss = adversial.train_on_batch(random_latent_vectors, real)\n",
    "#         discriminator.trainable = True # unfreeze the discrminator\n",
    "       \n",
    "    \n",
    "        # Plot the progress\n",
    "#         print (\"%d [D loss: %f, acc.: %.2f%%] [A loss: %f]\" % (epoch, d_loss[0], 100*d_loss[1], a_loss))\n",
    "        log_mesg = \"%d: [D loss: %f, acc: %f]\" % (epoch, d_loss[0], d_loss[1])\n",
    "        log_mesg = \"%s  [A loss: %f, acc: %f]\" % (log_mesg, a_loss[0], a_loss[1])\n",
    "        print(log_mesg)\n",
    "                       \n",
    "        # Saving sample generated images\n",
    "        if save_interval>0:\n",
    "            step = epoch+1\n",
    "            if step%save_interval==0:\n",
    "                filename = \"mnist_%d.png\" % step\n",
    "                images = generator.predict(noise_input)\n",
    "                plt.figure(figsize=(10,10))\n",
    "                for i in range(images.shape[0]):\n",
    "                    plt.subplot(4, 4, i+1)\n",
    "                    image = images[i, :, :, :]\n",
    "                    image = np.reshape(image, [img_cols, img_rows])\n",
    "                    plt.imshow(image, cmap='gray')\n",
    "                    plt.axis('off')\n",
    "                plt.tight_layout()\n",
    "                plt.savefig(filename)\n",
    "                plt.close('all')\n",
    "train(epochs = 4000, batch_size = 32, save_interval = 50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
