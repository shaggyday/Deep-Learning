{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From <ipython-input-1-992106301704>:91: read_data_sets (from tensorflow.contrib.learn.python.learn.datasets.mnist) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please use alternatives such as official/mnist/dataset.py from tensorflow/models.\n",
      "WARNING:tensorflow:From C:\\Users\\shaggyday\\Anaconda3\\lib\\site-packages\\tensorflow\\contrib\\learn\\python\\learn\\datasets\\mnist.py:260: maybe_download (from tensorflow.contrib.learn.python.learn.datasets.base) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please write your own downloading logic.\n",
      "WARNING:tensorflow:From C:\\Users\\shaggyday\\Anaconda3\\lib\\site-packages\\tensorflow\\contrib\\learn\\python\\learn\\datasets\\mnist.py:262: extract_images (from tensorflow.contrib.learn.python.learn.datasets.mnist) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please use tf.data to implement this functionality.\n",
      "Extracting mnist\\train-images-idx3-ubyte.gz\n",
      "WARNING:tensorflow:From C:\\Users\\shaggyday\\Anaconda3\\lib\\site-packages\\tensorflow\\contrib\\learn\\python\\learn\\datasets\\mnist.py:267: extract_labels (from tensorflow.contrib.learn.python.learn.datasets.mnist) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please use tf.data to implement this functionality.\n",
      "Extracting mnist\\train-labels-idx1-ubyte.gz\n",
      "WARNING:tensorflow:From C:\\Users\\shaggyday\\Anaconda3\\lib\\site-packages\\tensorflow\\contrib\\learn\\python\\learn\\datasets\\mnist.py:110: dense_to_one_hot (from tensorflow.contrib.learn.python.learn.datasets.mnist) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please use tf.one_hot on tensors.\n",
      "Extracting mnist\\t10k-images-idx3-ubyte.gz\n",
      "Extracting mnist\\t10k-labels-idx1-ubyte.gz\n",
      "WARNING:tensorflow:From C:\\Users\\shaggyday\\Anaconda3\\lib\\site-packages\\tensorflow\\contrib\\learn\\python\\learn\\datasets\\mnist.py:290: DataSet.__init__ (from tensorflow.contrib.learn.python.learn.datasets.mnist) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please use alternatives such as official/mnist/dataset.py from tensorflow/models.\n",
      "WARNING:tensorflow:From C:\\Users\\shaggyday\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\framework\\op_def_library.py:263: colocate_with (from tensorflow.python.framework.ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Colocations handled automatically by placer.\n",
      "WARNING:tensorflow:From C:\\Users\\shaggyday\\Anaconda3\\lib\\site-packages\\keras\\backend\\tensorflow_backend.py:3445: calling dropout (from tensorflow.python.ops.nn_ops) with keep_prob is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please use `rate` instead of `keep_prob`. Rate should be set to `rate = 1 - keep_prob`.\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "conv2d_1 (Conv2D)            (None, 14, 14, 64)        4160      \n",
      "_________________________________________________________________\n",
      "leaky_re_lu_1 (LeakyReLU)    (None, 14, 14, 64)        0         \n",
      "_________________________________________________________________\n",
      "dropout_1 (Dropout)          (None, 14, 14, 64)        0         \n",
      "_________________________________________________________________\n",
      "conv2d_2 (Conv2D)            (None, 7, 7, 128)         524416    \n",
      "_________________________________________________________________\n",
      "leaky_re_lu_2 (LeakyReLU)    (None, 7, 7, 128)         0         \n",
      "_________________________________________________________________\n",
      "dropout_2 (Dropout)          (None, 7, 7, 128)         0         \n",
      "_________________________________________________________________\n",
      "conv2d_3 (Conv2D)            (None, 4, 4, 256)         2097408   \n",
      "_________________________________________________________________\n",
      "leaky_re_lu_3 (LeakyReLU)    (None, 4, 4, 256)         0         \n",
      "_________________________________________________________________\n",
      "dropout_3 (Dropout)          (None, 4, 4, 256)         0         \n",
      "_________________________________________________________________\n",
      "conv2d_4 (Conv2D)            (None, 4, 4, 512)         8389120   \n",
      "_________________________________________________________________\n",
      "leaky_re_lu_4 (LeakyReLU)    (None, 4, 4, 512)         0         \n",
      "_________________________________________________________________\n",
      "dropout_4 (Dropout)          (None, 4, 4, 512)         0         \n",
      "_________________________________________________________________\n",
      "flatten_1 (Flatten)          (None, 8192)              0         \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 1)                 8193      \n",
      "_________________________________________________________________\n",
      "activation_1 (Activation)    (None, 1)                 0         \n",
      "=================================================================\n",
      "Total params: 11,023,297\n",
      "Trainable params: 11,023,297\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "dense_2 (Dense)              (None, 12544)             1266944   \n",
      "_________________________________________________________________\n",
      "batch_normalization_1 (Batch (None, 12544)             50176     \n",
      "_________________________________________________________________\n",
      "leaky_re_lu_5 (LeakyReLU)    (None, 12544)             0         \n",
      "_________________________________________________________________\n",
      "reshape_1 (Reshape)          (None, 7, 7, 256)         0         \n",
      "_________________________________________________________________\n",
      "dropout_5 (Dropout)          (None, 7, 7, 256)         0         \n",
      "_________________________________________________________________\n",
      "up_sampling2d_1 (UpSampling2 (None, 14, 14, 256)       0         \n",
      "_________________________________________________________________\n",
      "conv2d_transpose_1 (Conv2DTr (None, 14, 14, 128)       2097280   \n",
      "_________________________________________________________________\n",
      "batch_normalization_2 (Batch (None, 14, 14, 128)       512       \n",
      "_________________________________________________________________\n",
      "leaky_re_lu_6 (LeakyReLU)    (None, 14, 14, 128)       0         \n",
      "_________________________________________________________________\n",
      "up_sampling2d_2 (UpSampling2 (None, 28, 28, 128)       0         \n",
      "_________________________________________________________________\n",
      "conv2d_transpose_2 (Conv2DTr (None, 28, 28, 64)        524352    \n",
      "_________________________________________________________________\n",
      "batch_normalization_3 (Batch (None, 28, 28, 64)        256       \n",
      "_________________________________________________________________\n",
      "leaky_re_lu_7 (LeakyReLU)    (None, 28, 28, 64)        0         \n",
      "_________________________________________________________________\n",
      "conv2d_transpose_3 (Conv2DTr (None, 28, 28, 32)        131104    \n",
      "_________________________________________________________________\n",
      "batch_normalization_4 (Batch (None, 28, 28, 32)        128       \n",
      "_________________________________________________________________\n",
      "leaky_re_lu_8 (LeakyReLU)    (None, 28, 28, 32)        0         \n",
      "_________________________________________________________________\n",
      "conv2d_transpose_4 (Conv2DTr (None, 28, 28, 1)         801       \n",
      "_________________________________________________________________\n",
      "activation_2 (Activation)    (None, 28, 28, 1)         0         \n",
      "=================================================================\n",
      "Total params: 4,071,553\n",
      "Trainable params: 4,046,017\n",
      "Non-trainable params: 25,536\n",
      "_________________________________________________________________\n",
      "WARNING:tensorflow:From C:\\Users\\shaggyday\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\ops\\math_ops.py:3066: to_int32 (from tensorflow.python.ops.math_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use tf.cast instead.\n",
      "0: [D loss: 0.692275, acc: 0.503906]  [A loss: 4.840490, acc: 0.000000]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1: [D loss: 1.317801, acc: 0.500000]  [A loss: 0.720518, acc: 0.082031]\n",
      "2: [D loss: 0.591307, acc: 0.832031]  [A loss: 0.896997, acc: 0.031250]\n",
      "3: [D loss: 1.056406, acc: 0.500000]  [A loss: 1.625587, acc: 0.000000]\n",
      "4: [D loss: 0.397837, acc: 0.705078]  [A loss: 0.483006, acc: 0.972656]\n",
      "5: [D loss: 0.263441, acc: 0.974609]  [A loss: 0.110946, acc: 1.000000]\n",
      "6: [D loss: 0.113274, acc: 0.980469]  [A loss: 0.006572, acc: 1.000000]\n",
      "7: [D loss: 0.185089, acc: 0.941406]  [A loss: 4.638953, acc: 0.000000]\n",
      "8: [D loss: 0.633718, acc: 0.701172]  [A loss: 0.001468, acc: 1.000000]\n",
      "9: [D loss: 3.716733, acc: 0.488281]  [A loss: 2.463829, acc: 0.000000]\n",
      "10: [D loss: 0.358277, acc: 0.796875]  [A loss: 0.484216, acc: 0.921875]\n",
      "11: [D loss: 0.242495, acc: 0.953125]  [A loss: 0.188000, acc: 1.000000]\n",
      "12: [D loss: 0.190870, acc: 0.974609]  [A loss: 0.109131, acc: 1.000000]\n",
      "13: [D loss: 0.167943, acc: 0.978516]  [A loss: 0.114903, acc: 0.992188]\n",
      "14: [D loss: 0.142169, acc: 0.976562]  [A loss: 0.034440, acc: 1.000000]\n",
      "15: [D loss: 0.201921, acc: 0.953125]  [A loss: 2.965605, acc: 0.003906]\n",
      "16: [D loss: 0.483097, acc: 0.777344]  [A loss: 0.017833, acc: 1.000000]\n",
      "17: [D loss: 0.771439, acc: 0.621094]  [A loss: 4.593955, acc: 0.000000]\n",
      "18: [D loss: 0.756733, acc: 0.619141]  [A loss: 0.287001, acc: 0.957031]\n",
      "19: [D loss: 0.318217, acc: 0.896484]  [A loss: 1.518948, acc: 0.152344]\n",
      "20: [D loss: 0.253293, acc: 0.896484]  [A loss: 0.087032, acc: 1.000000]\n",
      "21: [D loss: 0.199859, acc: 0.953125]  [A loss: 0.190723, acc: 0.960938]\n",
      "22: [D loss: 0.230146, acc: 0.925781]  [A loss: 0.501866, acc: 0.734375]\n",
      "23: [D loss: 0.294401, acc: 0.873047]  [A loss: 1.083565, acc: 0.351562]\n",
      "24: [D loss: 0.353053, acc: 0.865234]  [A loss: 1.548949, acc: 0.125000]\n",
      "25: [D loss: 0.362025, acc: 0.851562]  [A loss: 1.997329, acc: 0.027344]\n",
      "26: [D loss: 0.412481, acc: 0.837891]  [A loss: 5.621067, acc: 0.000000]\n",
      "27: [D loss: 0.706708, acc: 0.644531]  [A loss: 0.000508, acc: 1.000000]\n",
      "28: [D loss: 4.261352, acc: 0.500000]  [A loss: 2.109980, acc: 0.000000]\n",
      "29: [D loss: 0.550774, acc: 0.531250]  [A loss: 1.126508, acc: 0.019531]\n",
      "30: [D loss: 0.507737, acc: 0.867188]  [A loss: 2.205762, acc: 0.000000]\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import time\n",
    "from tensorflow.examples.tutorials.mnist import input_data\n",
    "\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Activation, Flatten, Reshape\n",
    "from keras.layers import Conv2D, Conv2DTranspose, UpSampling2D\n",
    "from keras.layers import LeakyReLU, Dropout\n",
    "from keras.layers import BatchNormalization\n",
    "from keras.optimizers import Adam, RMSprop\n",
    "from keras.datasets import mnist\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "kernel_size = 8\n",
    "\n",
    "def generator_model():\n",
    "    dropout = 0.4\n",
    "    dim = 7\n",
    "    depth = 256\n",
    "    # In: 100\n",
    "    # Out: 7 x 7 x 256\n",
    "    model = Sequential()\n",
    "    model.add(Dense(dim*dim*depth, input_dim=100))\n",
    "    model.add(BatchNormalization(momentum=0.9))\n",
    "    model.add(LeakyReLU())\n",
    "    model.add(Reshape((dim, dim, depth)))\n",
    "    model.add(Dropout(dropout))\n",
    "    # In: 7 x 7 x 256\n",
    "    # Out: 14 x 14 x 128\n",
    "    model.add(UpSampling2D())\n",
    "    model.add(Conv2DTranspose(int(depth/2), kernel_size, padding='same'))\n",
    "    model.add(BatchNormalization(momentum=0.9))\n",
    "    model.add(LeakyReLU())\n",
    "    # In: 14 x 14 x 128\n",
    "    # Out: 28 x 28 x 64\n",
    "    model.add(UpSampling2D())\n",
    "    model.add(Conv2DTranspose(int(depth/4), kernel_size, padding='same'))\n",
    "    model.add(BatchNormalization(momentum=0.9))\n",
    "    model.add(LeakyReLU())\n",
    "    # In: 28 x 28 x 64\n",
    "    # Out: 28 x 28 x 1\n",
    "    model.add(Conv2DTranspose(int(depth/8), kernel_size, padding='same'))\n",
    "    model.add(BatchNormalization(momentum=0.9))\n",
    "    model.add(LeakyReLU())\n",
    "    model.add(Conv2DTranspose(1, 5, padding='same'))\n",
    "    model.add(Activation('sigmoid'))\n",
    "    model.summary()\n",
    "    return model\n",
    "\n",
    "    # Out: 28 x 1\n",
    "\n",
    "\n",
    "def discriminator_model():\n",
    "    depth = 64\n",
    "    dropout = 0.4\n",
    "    model = Sequential()\n",
    "    # In: 28 x 28 x 1, depth = 1\n",
    "    # Out: 14 x 14 x 1, depth = 64\n",
    "    model.add(Conv2D(depth, kernel_size, strides = 2, padding ='same', input_shape=(28, 28, 1)))\n",
    "    model.add(LeakyReLU(alpha=0.2))\n",
    "    model.add(Dropout(dropout))\n",
    "    \n",
    "    model.add(Conv2D(depth*2, kernel_size, strides = 2, padding ='same'))\n",
    "    model.add(LeakyReLU(alpha=0.2))\n",
    "    model.add(Dropout(dropout))\n",
    "    \n",
    "    model.add(Conv2D(depth*4, kernel_size, strides = 2, padding ='same'))\n",
    "    model.add(LeakyReLU(alpha=0.2))\n",
    "    model.add(Dropout(dropout))\n",
    "    \n",
    "    model.add(Conv2D(depth*8, kernel_size, strides = 1, padding ='same'))\n",
    "    model.add(LeakyReLU(alpha=0.2))\n",
    "    model.add(Dropout(dropout))\n",
    "    \n",
    "    # Out: 1-dim probability\n",
    "    model.add(Flatten())\n",
    "    model.add(Dense(1))\n",
    "    model.add(Activation('sigmoid'))\n",
    "    model.summary()\n",
    "    return model\n",
    "\n",
    "def adversial_model(generator, discriminator):\n",
    "    model = Sequential()\n",
    "    model.add(generator)\n",
    "    discriminator.trainable = False\n",
    "    model.add(discriminator)\n",
    "    return model\n",
    "\n",
    "def train(epochs = 100, batch_size = 256, save_interval = 20):\n",
    "    # Initialize data and models\n",
    "    (X_train, y_train), (X_test, y_test) = mnist.load_data()\n",
    "    X_train = input_data.read_data_sets(\"mnist\",one_hot=True).train.images\n",
    "    X_train = X_train.reshape(-1, 28, 28, 1).astype(np.float32)\n",
    "    discriminator = discriminator_model()\n",
    "    generator = generator_model()\n",
    "    adversial = adversial_model(generator, discriminator) # discriminator is frozen\n",
    "    d_optim = RMSprop(lr=0.0002, decay=6e-8)\n",
    "    g_optim = RMSprop(lr=0.0001, decay=3e-8)\n",
    "#     generator.compile(loss='binary_crossentropy', optimizer=\"RMSprop\")\n",
    "    adversial.compile(loss='binary_crossentropy', optimizer=g_optim, metrics=['accuracy'])\n",
    "    discriminator.trainable = True # unfreeze discriminator\n",
    "    discriminator.compile(loss='binary_crossentropy', optimizer=d_optim, metrics=['accuracy'])\n",
    "    \n",
    "    # Stuff for saving images\n",
    "    filename = 'mnist.png'\n",
    "    noise_input = None\n",
    "    if save_interval > 0:\n",
    "        noise_input = np.random.uniform(-1, 1, size=[16,100])\n",
    "    \n",
    "    # The training loop\n",
    "    for epoch in range(epochs):\n",
    "        real_images = X_train[np.random.randint(0, X_train.shape[0], size=batch_size), :, :, :]\n",
    "        noise = np.random.uniform(-1, 1, size=[batch_size, 100])\n",
    "        fake_images = generator.predict(noise)\n",
    "        combined_images = np.concatenate((real_images, fake_images))\n",
    "        labels = np.ones([2*batch_size, 1])\n",
    "        labels[batch_size:, :] = 0\n",
    "#         labels = np.concatenate([np.ones((batch_size, 1)), np.zeros((batch_size, 1))])\n",
    "#         labels += 0.05 * np.random.random(labels.shape)\n",
    "        d_loss = discriminator.train_on_batch(combined_images, labels)\n",
    "                                        \n",
    "        misleading_targets = np.ones([batch_size, 1])\n",
    "        random_latent_vectors = np.random.uniform(-1, 1, size=[batch_size, 100])\n",
    "        discriminator.trainable = False # freeze the discriminator\n",
    "        a_loss = adversial.train_on_batch(random_latent_vectors, misleading_targets)\n",
    "        discriminator.trainable = True # unfreeze the discrminator\n",
    "                                        \n",
    "        log_mesg = \"%d: [D loss: %f, acc: %f]\" % (epoch, d_loss[0], d_loss[1])\n",
    "        log_mesg = \"%s  [A loss: %f, acc: %f]\" % (log_mesg, a_loss[0], a_loss[1])\n",
    "        print(log_mesg)\n",
    "                       \n",
    "        # Saving sample generated images\n",
    "        if save_interval>0:\n",
    "            step = epoch+1\n",
    "            if step%save_interval==0:\n",
    "                filename = \"mnist_%d.png\" % step\n",
    "                images = generator.predict(noise_input)\n",
    "                plt.figure(figsize=(10,10))\n",
    "                for i in range(images.shape[0]):\n",
    "                    plt.subplot(4, 4, i+1)\n",
    "                    image = images[i, :, :, :]\n",
    "                    image = np.reshape(image, [28, 28])\n",
    "                    plt.imshow(image, cmap='gray')\n",
    "                    plt.axis('off')\n",
    "                plt.tight_layout()\n",
    "                plt.savefig(filename)\n",
    "                plt.close('all')\n",
    "train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
